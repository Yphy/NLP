{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dart 전자공시의 사업보고서 중  '이사의 경영진단 및 분석의견' 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 프로젝트의 데이터는 kospi200기업들의 10년치 사업보고서 중 텍스트 분석을 하기 위해 '이사의 경영진단 및 분석의견' 파트만을 크롤링하여 만들었습니다. 전자공시 사이트에서 문서검색에 도움을 주는 api를 제공하기 때문에 input으로 필요한 기업의 종목코드를 row로 가지고 있는 데이터프레임을 넣을 시 dart에서 제공하는 사업보고서에 대한 정보와 그 내용을 데이터프레임 형식으로 산출하는 함수를 만들었습니다. \n",
    "\n",
    "사업보고서의 url은 일관적으로 'https://dart.fss.or.kr/dsaf001/main.do?rcpNo=' + rcp_no 의 형식을 취하기 때문에 발급받은 개인 api 키를 이용하여 해당기업의 rcp_no를 가져오고 , html 태그를 검색해 최종적으로 '이사의 경영진단 및 분석의견' 파트의 url 만  가져오는 식으로 만들었고, 다시 본문 내용을 request하여 최종적으로 텍스트를 저장합니다. 이 문서에서는 전처리가 포함되지않은 크롤링 작업만 하고 있습니다.\n",
    "\n",
    "api는 json과 xml의 형식을 제공하며 json으로 크롤링했을 때 실제 남은 사업보고서가 존재하지만 6-7년치 데이터만 크롤링되는 경우가 있었습니다. 따라서 코드에는 존재하지 않지만 xml과 json으로 모두 request하고 비교하여 채워넣는 방법을 택했습니다.\n",
    "\n",
    "Dart에서 1분에 100건 이상 검색을 할 시 사이트를 24시간동안 차단하고 있습니다. 이를 피하기 위해 user-agent를 추가하고 time sleep을 줘봤지만 차단당하는 경우가 많아서 파티션을 나눠서 크롤링하였습니다. \n",
    "\n",
    "아래는 예시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ./data/crawling/preprocessed/df.csv does not exist: './data/crawling/preprocessed/df.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0f97aff5bf69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/crawling/preprocessed/df.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File ./data/crawling/preprocessed/df.csv does not exist: './data/crawling/preprocessed/df.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/crawling/preprocessed/df.csv',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_code = 'api_code' # 개인 api 키를 발급받아서 사용\n",
    "start_date = '20100101'\n",
    "end_date = '20191231'\n",
    "crp_code = ''\n",
    "page_set = '100'\n",
    "url = 'http://dart.fss.or.kr/api/search.xml?auth='+api_code+'&crp_cd='+crp_code+'&start_dt='+start_date+'&bsn_tp=A001&page_set=' + page_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(crp_code): #검색된 회사의 사업보고서 내용을 데이터프레임으로 저장\n",
    "  \n",
    "    url = 'http://dart.fss.or.kr/api/search.xml?auth='+api_code+'&crp_cd='+crp_code+'&start_dt='+start_date+'&bsn_tp=A001&page_set=' + page_set\n",
    "    headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit 537.36 (KHTML, like Gecko) Chrome\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"}\n",
    "    \n",
    "    req = requests.get(url,headers = headers)\n",
    "    html = req.text\n",
    "    xmlsoup = BeautifulSoup(html, 'html.parser')\n",
    "    te = xmlsoup.findAll('list')\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    for t in te:\n",
    "                temp = pd.DataFrame(([[t.crp_cls.string, t.crp_nm.string, t.crp_cd.string, t.rpt_nm.string,\n",
    "                                  t.rcp_no.string, t.flr_nm.string, t.rcp_dt.string, t.rmk.string]]),\n",
    "                                  columns = [\"crp_cls\", \"crp_nm\", \"crp_cd\", \"rpt_nm\", \"rcp_no\", \"flr_nm\", \"rcp_dt\", \"rmk\"])\n",
    "                data = pd.concat([data,temp])\n",
    "    data.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    return(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a3TNVQdna26m"
   },
   "outputs": [],
   "source": [
    "def get_df2(crp_code): #검색된 회사의 사업보고서 내용을 데이터프레임으로 저장\n",
    "  \n",
    "    url = 'http://dart.fss.or.kr/api/search.xml?auth='+api_code+'&crp_cd='+crp_code+'&start_dt='+start_date+'&bsn_tp=A001&page_set=' + page_set\n",
    "    xmlsoup = BeautifulSoup(urlopen(url).read(), 'html.parser')\n",
    "    te = xmlsoup.findAll('list')\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    for t in te:\n",
    "                temp = pd.DataFrame(([[t.crp_cls.string, t.crp_nm.string, t.crp_cd.string, t.rpt_nm.string,\n",
    "                                  t.rcp_no.string, t.flr_nm.string, t.rcp_dt.string, t.rmk.string]]),\n",
    "                                  columns = [\"crp_cls\", \"crp_nm\", \"crp_cd\", \"rpt_nm\", \"rcp_no\", \"flr_nm\", \"rcp_dt\", \"rmk\"])\n",
    "                data = pd.concat([data,temp])\n",
    "    data.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    return(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UnOs2ly7a238"
   },
   "outputs": [],
   "source": [
    "def get_url(data): #dataframe을 넣었을 때 해당 사업보고서의 주소를 저장\n",
    "    rcp_ls=[]\n",
    "    for row in data['rcp_no']:\n",
    "        rcp_ls.append(row)\n",
    "        urls = []\n",
    "        for i in rcp_ls:\n",
    "            urls.append('https://dart.fss.or.kr/dsaf001/main.do?rcpNo='+i)\n",
    "    \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mk2SCGLNa21G"
   },
   "outputs": [],
   "source": [
    "def final_url(url): #사업보고서 주소를 넣으면 '이사의 경영진단 및 분석' 주소 반환\n",
    "    try:\n",
    "        req2 = requests.get(url)\n",
    "        html2 = req2.text\n",
    "        soup2 = BeautifulSoup(html2, 'html.parser')\n",
    "        body = str(soup2.find('head'))\n",
    "\n",
    "        a = re.search('이사의 경영진단 및 분석의견',body).span()\n",
    "        b= re.search(r'viewDoc(.*);',body[a[0]:]).group()\n",
    "        list = b[8:-2].split(',')\n",
    "        list = [i[1:-1] for i in list]\n",
    "        list[1] = list[1][1:] #드러움\n",
    "        list[2] = list[2][1:]\n",
    "        list[3] = list[3][1:]\n",
    "        list[4] = list[4][1:]\n",
    "        list[5] = list[5][1:]    \n",
    "\n",
    "        url_final = 'http://dart.fss.or.kr/report/viewer.do?rcpNo='+ list[0] + '&dcmNo='+ list[1]+'&eleId=' +list[2] + '&offset=' + list[3] + '&length=' + list[4] + '&dtd=dart3.xsd'\n",
    "        return(url_final)\n",
    "    \n",
    "    except AttributeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtKje2IXa2vz"
   },
   "outputs": [],
   "source": [
    "def extracting_text(url): #경영진단 url 넣었을 시 해당 본문 텍스트 반환\n",
    "    req3 = requests.get(url)\n",
    "    html3 = req3.text\n",
    "    soup3 = BeautifulSoup(html3,'html.parser')\n",
    "    tables = []\n",
    "    ka = soup3.find_all('p') #table을 제외한 본문 text 부분 찾아서 리스트안에저장\n",
    "    for k in ka:\n",
    "        tables.append(k.get_text())\n",
    "    tables = tables[4:] #개요부분 삭제\n",
    "    table = ''.join(tables) #리스트를 string으로 바꾼다\n",
    "    table = table.replace('\\xa0','') #정리\n",
    "    table = table.replace('\\n','')\n",
    "    # table = re.sub('[[a-zA-Z]',\"\",table) #전처리\n",
    "    # table = re.sub('[-=+,#/\\?:^$@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]','',table)\n",
    "    #table = re.sub('[0-9]+','num',table) #숫자를 special token num으로 치환\n",
    "    # pattern = re.compile(r'\\s+') #중복띄어쓰기 제거\n",
    "    # table = re.sub(pattern,' ',table)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SgS5tHJBa2tf"
   },
   "outputs": [],
   "source": [
    "def final(crp_code): #기업의 종목 코드 넣었을 시 근 10개년 종목내용을 저장한 dataframe 반환\n",
    "    try:\n",
    "        df = get_df(crp_code)\n",
    "        urls = get_url(df)\n",
    "        strs=[]\n",
    "        for i in urls:\n",
    "            strs.append(extracting_text(final_url(i)))\n",
    "        str_series = pd.Series(strs)\n",
    "        df['str']= str_series\n",
    "        return(df)\n",
    "    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(df): #종목코드가 들어간 리스트가 있는 데이터프레임을 넣었을 시 str이 모두 추출된 데이터프레임 반환\n",
    "    # start_vect=time.time()                                                                                                                                                                 \n",
    "    ls = []\n",
    "    body = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        for i in df['Symbol']:\n",
    "            ls.append(str(i).zfill(6))\n",
    "        for process,i in enumerate(ls):\n",
    "            \n",
    "            print(\"Process : {} | Total : {}\".format(process+1,len(ls)),end = '\\r')\n",
    "            temp = final(i)\n",
    "            body = pd.concat([body,temp],ignore_index=True)\n",
    "            \n",
    "            rand_value = randint(1, MAX_SLEEP_TIME)\n",
    "           \n",
    "  \n",
    "        \n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "    \n",
    "    # print(\"training Runtime: %0.2f Minutes\"%((time.time() - start_vect)/60))\n",
    "    return(body)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMSPHpb6sw7YCTKHM1H1uAj",
   "collapsed_sections": [],
   "name": "Dart crawling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
